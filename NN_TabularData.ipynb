{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6uTrkdXZ-5x"
   },
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDaRzuTsZ-53"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sys\n",
    "from csv import writer\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eFWlnF2Z-55"
   },
   "source": [
    "### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "id_7D1UzZ-56"
   },
   "outputs": [],
   "source": [
    "def dataset_read(featurelist):\n",
    "    data = pd.read_csv('heart_failure.csv')\n",
    "    data = data[featurelist]\n",
    "    #Normalization\n",
    "    data=np.array(data)\n",
    "    input_data_normed = data / data.max(axis=0)\n",
    "    #print(input_data_normed.shape)\n",
    "    return input_data_normed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9VoAZRHZ-57"
   },
   "source": [
    "### Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDERhzFrZ-57"
   },
   "outputs": [],
   "source": [
    "# 85/15 train/test split\n",
    "\n",
    "def dataset_split(input_data_normed):\n",
    "    training, test = input_data_normed[:int(.85*len(input_data_normed)),:], input_data_normed[int(.85*len(input_data_normed)):,:]\n",
    "    #label_training = training[:, 0]\n",
    "    #label_test = test[:, 0]\n",
    "    #test = np.delete(test, 0, 1)\n",
    "    return training,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8KV0avDZ-58"
   },
   "outputs": [],
   "source": [
    "def training_1(training,test, splitting_ratio):\n",
    "    #np.seed\n",
    "    #np.random.shuffle(training)\n",
    "    train,valid = training[:int(splitting_ratio*len(training)),:], training[int(splitting_ratio*len(training)):,:]\n",
    "    label_train, label_valid = train[:, 0], valid[:, 0]\n",
    "    train = np.delete(train, 0, 1)\n",
    "    valid = np.delete(valid, 0, 1)\n",
    "    \n",
    "    #print(train.shape,label_train.shape)\n",
    "\n",
    "    #Run Logistic Regression\n",
    "    #clf=SVC(kernel='rbf')\n",
    "    #clf = LogisticRegression(random_state=0)\n",
    "    clf = MLPClassifier(batch_size=32,learning_rate_init=0.001,hidden_layer_sizes=32,random_state=42,activation= 'relu', max_iter=2000)\n",
    "    clf.fit(train, label_train)\n",
    "\n",
    "    label_test = test[:, 0]\n",
    "    test = np.delete(test, 0, 1)\n",
    "    \n",
    "    #print(test.shape,label_test.shape)\n",
    "    y_pred=clf.predict(test)\n",
    "    result = accuracy_score(label_test, y_pred)\n",
    "    return result\n",
    "    #print(\"Test Score: \", accuracy_score(label_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZys01BEZ-5-"
   },
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TOvWY-VZ-5-",
    "outputId": "5003da3c-b61c-4831-ed8a-a27b27a5a7dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Main Function, run for only one feature set\n",
    "\n",
    "cur_feature = ['Event', 'TIME', 'Gender', 'Smoking']\n",
    "\n",
    "input_data_normed = dataset_read(cur_feature)\n",
    "train,test = dataset_split(input_data_normed)\n",
    "\n",
    "training_1(train,test,0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DrDeoRzZ-6A"
   },
   "source": [
    "### FeatureSet selection (dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snuVZdeqZ-6A"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('heart_failure.csv')\n",
    "all_features = data.columns.tolist()\n",
    "all_features.remove('Event')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whd1MiC9Z-6B",
    "outputId": "1f31d518-c88f-42a0-9abc-618e1b584366"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def powerset(s):\n",
    "    x = len(s)\n",
    "    all_feature_set = []\n",
    "    for i in range(1 << x):\n",
    "        cur_list = [s[j] for j in range(x) if (i & (1 << j))]\n",
    "        #print(cur_list)\n",
    "        all_feature_set.append(cur_list)\n",
    "    return all_feature_set\n",
    "\n",
    "all_feature_set = powerset(all_features)\n",
    "len(all_feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LD4OZPJkZ-6B"
   },
   "outputs": [],
   "source": [
    "def write_final_data(filename, List):\n",
    "\n",
    "    # Open our existing CSV file in append mode\n",
    "    # Create a file object for this file\n",
    "    with open(filename, 'a') as f_object:\n",
    "\n",
    "        # Pass this file object to csv.writer()\n",
    "        # and get a writer object\n",
    "        writer_object = writer(f_object)\n",
    "\n",
    "        # Pass the list as an argument into\n",
    "        # the writerow()\n",
    "        writer_object.writerow(List)\n",
    "\n",
    "        #Close the file object\n",
    "        f_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kK3QIFNIZ-6C"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "splitting_ratio = .85\n",
    "\n",
    "for i in range(0, 3):\n",
    "    #filename = \"finalResult.csv\"\n",
    "    filename ='finalResultNN_' + str(i+1) + '.csv'\n",
    "    \n",
    "    # opening the file with w+ mode truncates the file\n",
    "    f = open(filename, \"w+\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    cnt = 0\n",
    "    for feature_set in all_feature_set:\n",
    "        cnt = cnt + 1\n",
    "        if(cnt==1):\n",
    "            continue\n",
    "        cur_feature_list = ['Event']+feature_set\n",
    "        #print(cur_feature_list)\n",
    "        input_data_normed = dataset_read(cur_feature_list)\n",
    "        train,test = dataset_split(input_data_normed)\n",
    "        result = training_1(train,test, splitting_ratio)\n",
    "        write_final_data(filename, [cur_feature_list, result])\n",
    "    \n",
    "    splitting_ratio = splitting_ratio + .05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-9--bp2Z-6C"
   },
   "source": [
    "### Read from CSV file to find the **maximum accuracy** and **best feautre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PnTf2LZEZ-6D"
   },
   "outputs": [],
   "source": [
    "def best_feature(data, maxAcc):\n",
    "    \n",
    "    best_feature = []\n",
    "    len_best_feature = 100000\n",
    "\n",
    "    for i in range(0, len(data), 1):\n",
    "        if data[i][1] == maxAcc:\n",
    "            best_feature.append(data[i][0])\n",
    "            if len(data[i][0]) < len_best_feature:\n",
    "                len_best_feature = len(data[i][0])\n",
    "                feature = data[i][0]\n",
    "    \n",
    "    print_function(feature, maxAcc, best_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1UN_SSuZ-6D"
   },
   "outputs": [],
   "source": [
    "def print_function(feature, maxAcc, best_feature):\n",
    "\n",
    "    print(\"Best Feature List\", feature)\n",
    "    print(\"Maximum Accuracy: \", maxAcc)\n",
    "    print()\n",
    "    #print(\"All Feature List: \\n\", best_feature)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNvOAkenZ-6E"
   },
   "outputs": [],
   "source": [
    "def best_feature_func():\n",
    "    for i in range(0, 3):\n",
    "        #filename = \"finalResult.csv\"\n",
    "        filename ='finalResultNN_' + str(i+1) + '.csv'\n",
    "        with open(filename, newline='') as f:\n",
    "            reader = csv.reader(f)\n",
    "            data = list(reader)\n",
    "\n",
    "        data = np.array(data)\n",
    "        data.shape\n",
    "\n",
    "        accList = []\n",
    "        for i in range(0, len(data), 1):\n",
    "            accList.append(data[i][1])\n",
    "\n",
    "        maxAcc = max(accList)\n",
    "        best_feature(data, maxAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6sxXWCEZ-6E",
    "outputId": "3d15c740-2716-4001-acb2-9985dda591ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Feature List ['Event', 'TIME', 'Smoking', 'Ejection.Fraction']\n",
      "Maximum Accuracy:  0.9555555555555556\n",
      "\n",
      "\n",
      "\n",
      "Best Feature List ['Event', 'TIME', 'Gender', 'Ejection.Fraction', 'Sodium']\n",
      "Maximum Accuracy:  0.9555555555555556\n",
      "\n",
      "\n",
      "\n",
      "Best Feature List ['Event', 'TIME', 'Smoking', 'BP', 'Ejection.Fraction']\n",
      "Maximum Accuracy:  0.9555555555555556\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_feature_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_nlZgRJZ-6F"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/heart_failure.csv')\n",
    "\n",
    "#Normalization\n",
    "data=np.array(data)\n",
    "input_data_normed = data / data.max(axis=0)\n",
    "\n",
    "training, test = input_data_normed[:int(.85*len(input_data_normed)),:], input_data_normed[int(.85*len(input_data_normed)):,:]\n",
    "label_training = training[:, 1]\n",
    "label_test = test[:, 1]\n",
    "test = np.delete(test, 1, 1)\n",
    "\n",
    "np.random.shuffle(training)\n",
    "train,valid = training[:int(.95*len(input_data_normed)),:], input_data_normed[int(.95*len(input_data_normed)):,:]\n",
    "label_train, label_valid = train[:, 1], valid[:, 1]\n",
    "train = np.delete(train, 1, 1)\n",
    "valid = np.delete(valid, 1, 1)\n",
    "\n",
    "#Run MLP Classifier\n",
    "clf = MLPClassifier(batch_size=32,learning_rate_init=0.001,hidden_layer_sizes=32,random_state=42,activation= 'relu', max_iter=2000).fit(train,label_train)\n",
    "print(\"Validation Score: \", clf.score(valid, label_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for combinations of hyper-parameters\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "\n",
    "\n",
    "expected_loss_list = []\n",
    "bias_list = []\n",
    "variance_list = []\n",
    "hidden_layer_sizesList_forGraph = []\n",
    "alphaList_forGraph = []\n",
    "\n",
    "minExpectedLoss = 1000000000000000\n",
    "best_hidden_layer_sizes_Value = -1\n",
    "best_alphaValue = -1\n",
    "# itr = 1\n",
    "for hidden_layer_sizes_Value in [100, 200, 300, 400]:\n",
    "  for alphaValue in [0.00001, 0.0001, 0.001, 0.01, 0.1]:\n",
    "    # print(itr)\n",
    "    # itr = itr + 1\n",
    "    clf=MLPClassifier(batch_size=32,learning_rate_init=0.001,hidden_layer_sizes=hidden_layer_sizes_Value, alpha = alphaValue, random_state=42,activation= 'relu', max_iter=2000)\n",
    "    expected_loss, bias, variance = bias_variance_decomp(\n",
    "        clf, train, label_train, valid, label_valid, \n",
    "        loss='0-1_loss', num_rounds = 400,\n",
    "        random_seed=123)\n",
    "    if expected_loss < minExpectedLoss:\n",
    "      minExpectedLoss = expected_loss\n",
    "      best_hidden_layer_sizes_Value = hidden_layer_sizes_Value\n",
    "      best_alphaValue = alphaValue\n",
    "    \n",
    "    # expected_loss_list.append(expected_loss)\n",
    "    # bias_list.append(bias)\n",
    "    # variance_list.append(variance)\n",
    "\n",
    "    hidden_layer_sizesList_forGraph.append(hidden_layer_sizes_Value)\n",
    "    alphaList_forGraph.append(alphaValue)\n",
    "\n",
    "    # print(expected_loss, bias, variance)\n",
    "\n",
    "print(\"Best hyperparameters:\", best_hidden_layer_sizes_Value, best_alphaValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for graph\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "\n",
    "\n",
    "\n",
    "expected_loss_list = []\n",
    "bias_list = []\n",
    "variance_list = []\n",
    "hidden_layer_sizesList_forGraph = []\n",
    "alphaList_forGraph = []\n",
    "\n",
    "# itr = 1\n",
    "# for cValue in np.arange(0.1, 2.1, 0.1):\n",
    "minExpectedLoss = 10000000000\n",
    "best_hidden_layer_sizes_Value = -1\n",
    "for hidden_layer_sizes_Value in [100, 200, 300, 400]:\n",
    "    # print(itr)\n",
    "    # itr = itr + 1\n",
    "    clf=MLPClassifier(batch_size=32,learning_rate_init=0.001,hidden_layer_sizes=hidden_layer_sizes_Value, alpha = 0.0001, random_state=42,activation= 'relu', max_iter=2000)\n",
    "    expected_loss, bias, variance = bias_variance_decomp(\n",
    "        clf, train, label_train, valid, label_valid, \n",
    "        loss='0-1_loss', num_rounds = 400,\n",
    "        random_seed=123)\n",
    "    if expected_loss < minExpectedLoss:\n",
    "      minExpectedLoss = expected_loss\n",
    "      best_hidden_layer_sizes_Value = hidden_layer_sizes_Value\n",
    "    \n",
    "    expected_loss_list.append(expected_loss)\n",
    "    bias_list.append(bias*bias)\n",
    "    variance_list.append(variance)\n",
    "\n",
    "    hidden_layer_sizesList_forGraph.append(hidden_layer_sizes_Value)\n",
    "    alphaList_forGraph.append(alphaValue)\n",
    "\n",
    "#draw graph\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "fig = plt.figure()\n",
    " \n",
    "# ax = plt.axes(projection ='3d')\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "# fig.suptitle('C=1.0')\n",
    "ax.plot(hidden_layer_sizesList_forGraph, expected_loss_list, 'red', label='Expected loss')\n",
    "ax.plot(hidden_layer_sizesList_forGraph, bias_list, 'green', label = 'Bias'r'$^2$')\n",
    "ax.plot(hidden_layer_sizesList_forGraph, variance_list, 'blue', label = 'Variance')\n",
    "# ax.plot3D(clist_forGraph, gammaList_forGraph, expected_loss_list, 'green')\n",
    "# ax.plot3D(clist_forGraph, gammaList_forGraph, bias_list, 'red')\n",
    "# ax.plot3D(clist_forGraph, gammaList_forGraph, variance_list, 'blue')\n",
    "\n",
    "ax.set_xlabel('hidden_layer_sizes')\n",
    "ax.set_ylabel('Loss value')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NN_TabularData-Copy1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
